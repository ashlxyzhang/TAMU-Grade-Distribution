{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import urllib3\n",
    "from urllib3.util.ssl_ import create_urllib3_context\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import tabula\n",
    "\n",
    "import logging\n",
    "from flask import Flask, request, render_template, session, redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url\n",
    "url = \"https://web-as.tamu.edu/gradereports/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom context\n",
    "ctx = create_urllib3_context()\n",
    "ctx.load_default_certs()\n",
    "ctx.options |= 0x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PoolManager instance to make requests\n",
    "http = urllib3.PoolManager(ssl_context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HTTPReponse object\n",
    "read = http.request(\"GET\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse HTML content using beautifulsoup\n",
    "html = read.data\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find filter elements\n",
    "year = soup.find(\"select\", {\"name\": \"ctl00$plcMain$lstGradYear\"})\n",
    "sem = soup.find(\"select\", {\"name\": \"ctl00$plcMain$lstGradTerm\"})\n",
    "college = soup.find(\"select\", {\"name\": \"ctl00$plcMain$lstGradCollege\"})\n",
    "\n",
    "# all years\n",
    "year_options = year.find_all(\"option\")\n",
    "year_list = [option[\"value\"] for option in year_options]\n",
    "year_list = year_list[0:4]\n",
    "\n",
    "# all sems (spring, summer, fall)\n",
    "sem_list = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "# all colleges\n",
    "college_options = college.find_all(\"option\")\n",
    "college_remove = [\"DN_PROF\", \"DT_PROF\", \"SL_PROF\", \"MD_PROF\", \"MN_PROF\", \"UT\"]\n",
    "college_list = [option[\"value\"] for option in college_options if option[\"value\"] not in college_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pdf urls\n",
    "base_url = \"https://web-as.tamu.edu/GradeReports/PDFReports/\"\n",
    "pdf_urls = []\n",
    "\n",
    "for year in year_list:\n",
    "    for sem in sem_list:\n",
    "        for col in college_list:\n",
    "            pdf_url = f\"{year}{sem}/grd{year}{sem}{col}.pdf\"\n",
    "            full_url = urljoin(base_url, pdf_url)\n",
    "            \n",
    "            response = http.request(\"HEAD\", full_url)\n",
    "\n",
    "            if response.status == 200 and response.headers['Content-Type'] == 'application/pdf':\n",
    "                pdf_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf metadata\n",
    "\n",
    "top = 100\n",
    "left = 30\n",
    "width = 720\n",
    "height = 500\n",
    "\n",
    "table_area = [top, left, top + height, left + width]\n",
    "table_x_coords = [130, 177, 222, 267, 314, 359, 404, 440, 473, 505, 537, 568, 600, 642, 750]\n",
    "\n",
    "top_d = 73\n",
    "left_d = 33\n",
    "w_d = 270\n",
    "h_d = 29\n",
    "\n",
    "c_area = [top_d, left_d, top_d + h_d, left_d + w_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(pdf):\n",
    "\n",
    "    # read all grade tables and departments in one college pdf\n",
    "    tables = tabula.read_pdf(pdf, pages = 'all', area=table_area, columns=table_x_coords)\n",
    "    deps = tabula.read_pdf(pdf, pages = 'all', area=c_area, pandas_options={'header': None})\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    tables = [table.dropna().drop(['I', 'S', 'U', 'X', 'A - F'], axis = 1) for table in tables]\n",
    "\n",
    "    # create A, B, C, D, and F percentages\n",
    "    convert = ['A', 'B', 'C', 'D', 'F']\n",
    "    for table in tables:\n",
    "        table[convert] = table[convert].astype(int)\n",
    "        for col in convert:\n",
    "            table[col + '_PER'] = round(table[col]/table['TOTAL'] * 100, 2)\n",
    "        \n",
    "        # split section into course\n",
    "        split_sec = table['SECTION'].str.split('-')\n",
    "        table['COURSE'] = split_sec.str[0] + \" \" + split_sec.str[1]\n",
    "    \n",
    "    tables = [table.drop('SECTION', axis = 1) for table in tables]\n",
    "    \n",
    "    if(len(tables) != len(deps)):\n",
    "        raise Exception(\"table lengths not matching\")\n",
    "    \n",
    "    relates = {}\n",
    "\n",
    "    # {d1 : t1, d2 : t2, d3, t3}\n",
    "    # relate each department to its respective table\n",
    "    for i in range(0, len(tables)):\n",
    "        department = deps[i][1][1]\n",
    "\n",
    "        relates[department] = pd.concat([relates.get(department), tables[i]], axis=0)\n",
    "        \n",
    "    return relates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grd = []\n",
    "done = False\n",
    "\n",
    "count = 0\n",
    "for url in pdf_urls:\n",
    "    try:\n",
    "        all_grd.append(get_tables(url))\n",
    "        print(f\"On URL: {count}\")\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        pdf_urls.remove(url)\n",
    "        logging.warning(f\"Exception: {type(e).__name__} : {e} --- URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_tables(pdf_urls[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARCHITECTURE', 'CONSTRUCTION SCIENCE', 'LAND ARCH & URBAN PLANNING', 'SCHOOL OF ARCHITECTURE']\n"
     ]
    }
   ],
   "source": [
    "print(list(t.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(t.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./grds/arch.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
